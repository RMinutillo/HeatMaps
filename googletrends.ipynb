{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "from googlesearch import search\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_titulos_peliculas():\n",
    "    url = \"https://trends.google.com/trends/trendingsearches/daily?geo=MX&hl=es\"\n",
    "\n",
    "    # Configurar Selenium\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Obtener la página con Selenium\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Esperar a que la página se cargue completamente (puedes ajustar el tiempo si es necesario)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, '[ng-repeat=\"titlePart in titleArray\"]'))\n",
    "        )\n",
    "\n",
    "        # Crear una lista para almacenar los datos\n",
    "        datos = []\n",
    "\n",
    "        # Intentar hacer clic en el botón \"Cargar más\" varias veces\n",
    "        for _ in range(1):  # Puedes ajustar la cantidad de clics según sea necesario\n",
    "            cargar_mas_button = driver.find_element(By.CLASS_NAME, 'feed-load-more-button')\n",
    "            if cargar_mas_button.is_displayed():\n",
    "                driver.execute_script(\"arguments[0].click();\", cargar_mas_button)\n",
    "                time.sleep(5)  # Esperar a que carguen los nuevos resultados\n",
    "            else:\n",
    "                break  # Si el botón no está visible, salimos del bucle\n",
    "\n",
    "        # Obtener el HTML después de cargar completamente\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Analizar el HTML de la página\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Encontrar los títulos de las películas\n",
    "        titulos = soup.select('[ng-repeat=\"titlePart in titleArray\"]')\n",
    "\n",
    "        for indice, titulo in enumerate(titulos, start=1):\n",
    "            titulo_texto = titulo.text.strip()\n",
    "            #print(f\"{indice}. {titulo_texto}\")\n",
    "\n",
    "            # Agregar los datos a la lista\n",
    "            datos.append([titulo_texto])\n",
    "\n",
    "        # Guardar los datos en un archivo CSV\n",
    "        with open('resultados.csv', mode='w', newline='', encoding='utf-8') as archivo_csv:\n",
    "            escritor_csv = csv.writer(archivo_csv)\n",
    "            escritor_csv.writerow(['Título'])  # Escribir encabezado\n",
    "            escritor_csv.writerows(datos)\n",
    "\n",
    "    finally:\n",
    "        # Cerrar el navegador\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obtener_titulos_peliculas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Service.__del__ at 0x0000015A843CCF40>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 185, in __del__\n",
      "    self.stop()\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 146, in stop\n",
      "    self.send_remote_shutdown_command()\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 126, in send_remote_shutdown_command\n",
      "    request.urlopen(f\"{self.service_url}/shutdown\")\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py\", line 215, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py\", line 515, in open\n",
      "    response = self._open(req, data)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py\", line 532, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py\", line 492, in _call_chain\n",
      "    result = func(*args)\n",
      "             ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py\", line 1373, in http_open\n",
      "    return self.do_open(http.client.HTTPConnection, req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py\", line 1344, in do_open\n",
      "    h.request(req.get_method(), req.selector, req.data, headers,\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1319, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1365, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1314, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1074, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 1018, in send\n",
      "    self.connect()\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py\", line 984, in connect\n",
      "    self.sock = self._create_connection(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ramir\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py\", line 844, in create_connection\n",
      "    exceptions.clear()  # raise only the last error\n",
      "    ^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# Lee el archivo csv generado arriba, ingresa a los link y corrobora palabras claves que le pasamos\n",
    "def buscar_palabras_clave_en_pagina(enlace, palabras_clave):\n",
    "    \"\"\"\n",
    "    Función para buscar palabras clave en el contenido de una página web.\n",
    "\n",
    "    :param enlace: El enlace de la página a analizar.\n",
    "    :param palabras_clave: Lista de palabras clave a buscar.\n",
    "    :return: True si al menos una palabra clave se encuentra en la página, False de lo contrario.\n",
    "    \"\"\"\n",
    "    # Configurar Selenium\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        # Obtener la página con Selenium\n",
    "        driver.get(enlace)\n",
    "\n",
    "        # Esperar a que la página se cargue completamente (puedes ajustar el tiempo si es necesario)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, 'body'))\n",
    "        )\n",
    "\n",
    "        # Obtener el contenido de la página\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Analizar el HTML de la página\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Obtener el texto del contenido de la página\n",
    "        contenido_pagina = soup.get_text().lower()\n",
    "\n",
    "        # Buscar palabras clave en el contenido de la página\n",
    "        for palabra_clave in palabras_clave:\n",
    "            if palabra_clave.lower() in contenido_pagina:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        # Cerrar el navegador\n",
    "        driver.quit()\n",
    "\n",
    "def obtener_titulos_desde_csv(csv_path, limite=5):\n",
    "    # Configurar Selenium\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        # Crear una lista para almacenar los datos\n",
    "        datos = []\n",
    "\n",
    "        with open(csv_path, mode='r', encoding='utf-8') as archivo_csv:\n",
    "            lector_csv = csv.reader(archivo_csv)\n",
    "            next(lector_csv)  # Saltar la primera fila (encabezado)\n",
    "\n",
    "            for indice, fila in enumerate(lector_csv, start=1):\n",
    "                enlace = fila[1]  # La columna 1 contiene los enlaces\n",
    "                titulo = fila[0]  # La columna 0 contiene los títulos\n",
    "\n",
    "                # Verificar si el contenido de la página contiene palabras clave\n",
    "                if buscar_palabras_clave_en_pagina(enlace, palabras_clave):\n",
    "                    # Agregar los datos a la lista\n",
    "                    datos.append([titulo, enlace])\n",
    "\n",
    "                # Detener el procesamiento después de las primeras 5 filas\n",
    "                if indice >= limite:\n",
    "                    break\n",
    "\n",
    "        # Imprimir los resultados\n",
    "        for resultado in datos:\n",
    "            print(\"Título:\", resultado[0])\n",
    "            print(\"Enlace:\", resultado[1])\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    finally:\n",
    "        # Cerrar el navegador\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    palabras_clave = [\"Messi\"]  # Personaliza las palabras clave según tus necesidades\n",
    "    csv_path = 'resultados.csv'  # Ruta del archivo CSV\n",
    "    limite_filas = 5  # Número de filas a procesar si queremos que chequee todo el csv le sacamos el limite\n",
    "\n",
    "    obtener_titulos_desde_csv(csv_path, limite=limite_filas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TITULO NOTICIAS Y ENLACES (PRUEBAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_titulos_noticias():\n",
    "    url = \"https://trends.google.com/trends/trendingsearches/daily?geo=MX&hl=es\"\n",
    "\n",
    "    # Configurar Selenium\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Obtener la página con Selenium\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Esperar a que la página se cargue completamente (puedes ajustar el tiempo si es necesario)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, '[ng-repeat=\"titlePart in titleArray\"]'))\n",
    "        )\n",
    "\n",
    "        # Crear una lista para almacenar los datos\n",
    "        datos = []\n",
    "\n",
    "        # Intentar hacer clic en el botón \"Cargar más\" varias veces\n",
    "        for _ in range(1):  # Puedes ajustar la cantidad de clics según sea necesario\n",
    "            cargar_mas_button = driver.find_element(By.CLASS_NAME, 'feed-load-more-button')\n",
    "            if cargar_mas_button.is_displayed():\n",
    "                driver.execute_script(\"arguments[0].click();\", cargar_mas_button)\n",
    "                time.sleep(5)  # Esperar a que carguen los nuevos resultados\n",
    "            else:\n",
    "                break  # Si el botón no está visible, salimos del bucle\n",
    "\n",
    "        # Obtener el HTML después de cargar completamente\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Analizar el HTML de la página\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Encontrar los títulos y enlaces de las noticias\n",
    "        noticias = soup.select('.summary-text a')\n",
    "\n",
    "        for indice, noticia in enumerate(noticias, start=1):\n",
    "            titulo_texto = noticia.find_parent('div', class_='details').find('div', class_='title').text.strip()\n",
    "            enlace = noticia['href']  # Obtener el enlace directamente del atributo href\n",
    "            # Agregar los datos a la lista\n",
    "            datos.append([titulo_texto, enlace])\n",
    "\n",
    "        # Guardar los datos en un archivo CSV\n",
    "        with open('resultados.csv', mode='w', newline='', encoding='utf-8') as archivo_csv:\n",
    "            escritor_csv = csv.writer(archivo_csv)\n",
    "            escritor_csv.writerow(['Título', 'Enlace'])  # Escribir encabezado\n",
    "            escritor_csv.writerows(datos)\n",
    "\n",
    "    finally:\n",
    "        # Cerrar el navegador\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obtener_titulos_noticias()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_palabras_clave_en_pagina(enlace, palabras_clave):\n",
    "    \"\"\"\n",
    "    Función para buscar palabras clave en el contenido de una página web.\n",
    "\n",
    "    :param enlace: El enlace de la página a analizar.\n",
    "    :param palabras_clave: Lista de palabras clave a buscar.\n",
    "    :return: True si al menos una palabra clave se encuentra en la página, False de lo contrario.\n",
    "    \"\"\"\n",
    "    # Configurar Selenium\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        # Obtener la página con Selenium\n",
    "        driver.get(enlace)\n",
    "\n",
    "        # Esperar a que la página se cargue completamente (puedes ajustar el tiempo si es necesario)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, 'body'))\n",
    "        )\n",
    "\n",
    "        # Obtener el contenido de la página\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Analizar el HTML de la página\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Buscar palabras clave en el contenido de la página\n",
    "        for palabra_clave in palabras_clave:\n",
    "            if palabra_clave.lower() in soup.get_text().lower():\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        # Cerrar el navegador\n",
    "        driver.quit()\n",
    "\n",
    "def obtener_titulos_noticias():\n",
    "    url = \"https://trends.google.com/trends/trendingsearches/daily?geo=MX&hl=es\"\n",
    "    palabras_clave = [\"México\", \"fútbol\"]  # Personaliza las palabras clave según tus necesidades\n",
    "\n",
    "    # Configurar Selenium\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        # Obtener la página con Selenium\n",
    "        driver.get(url)\n",
    "\n",
    "        # Esperar a que la página se cargue completamente (puedes ajustar el tiempo si es necesario)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, '[ng-repeat=\"titlePart in titleArray\"]'))\n",
    "        )\n",
    "\n",
    "        # Crear una lista para almacenar los datos\n",
    "        datos = []\n",
    "\n",
    "        # Contador para limitar el número de iteraciones\n",
    "        iteraciones = 0\n",
    "\n",
    "         # Intentar hacer clic en el botón \"Cargar más\" varias veces\n",
    "        for _ in range(1):  # Puedes ajustar la cantidad de clics según sea necesario\n",
    "            cargar_mas_button = driver.find_element(By.CLASS_NAME, 'feed-load-more-button')\n",
    "            if cargar_mas_button.is_displayed():\n",
    "                driver.execute_script(\"arguments[0].click();\", cargar_mas_button)\n",
    "                time.sleep(5)  # Esperar a que carguen los nuevos resultados\n",
    "            else:\n",
    "                break  # Si el botón no está visible, salimos del bucle\n",
    "            \n",
    "            # Obtener el HTML después de cargar completamente\n",
    "            page_source = driver.page_source\n",
    "\n",
    "            # Analizar el HTML de la página\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Encontrar los títulos y enlaces de las noticias\n",
    "            noticias = soup.select('.summary-text a')\n",
    "\n",
    "            for noticia in noticias:\n",
    "                titulo_texto = noticia.find_parent('div', class_='details').find('div', class_='title').text.strip()\n",
    "                enlace = noticia['href']  # Obtener el enlace directamente del atributo href\n",
    "\n",
    "                # Verificar si el enlace contiene palabras clave y si la noticia ya está en la lista\n",
    "                if buscar_palabras_clave_en_pagina(enlace, palabras_clave) and [titulo_texto, enlace] not in datos:\n",
    "                    # Agregar los datos a la lista\n",
    "                    datos.append([titulo_texto, enlace])\n",
    "\n",
    "            # Incrementar el contador de iteraciones\n",
    "            iteraciones += 1\n",
    "\n",
    "            # Limitar el número de iteraciones (ajusta según sea necesario)\n",
    "            if iteraciones >= 2:\n",
    "                break\n",
    "\n",
    "        # Imprimir los resultados\n",
    "        for resultado in datos:\n",
    "            print(\"Título:\", resultado[0])\n",
    "            print(\"Enlace:\", resultado[1])\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    finally:\n",
    "        # Cerrar el navegador\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obtener_titulos_noticias()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_palabras_clave_en_pagina(enlace, palabras_clave):\n",
    "    \"\"\"\n",
    "    Función para buscar palabras clave en el contenido de una página web.\n",
    "\n",
    "    :param enlace: El enlace de la página a analizar.\n",
    "    :param palabras_clave: Lista de palabras clave a buscar.\n",
    "    :return: True si al menos una palabra clave se encuentra en la página, False de lo contrario.\n",
    "    \"\"\"\n",
    "    # Configurar Selenium\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        # Obtener la página con Selenium\n",
    "        driver.get(enlace)\n",
    "\n",
    "        # Esperar a que la página se cargue completamente (puedes ajustar el tiempo si es necesario)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, 'body'))\n",
    "        )\n",
    "\n",
    "        # Obtener el contenido de la página\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Analizar el HTML de la página\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Obtener el texto del contenido de la página\n",
    "        contenido_pagina = soup.get_text().lower()\n",
    "\n",
    "        # Buscar palabras clave en el contenido de la página\n",
    "        for palabra_clave in palabras_clave:\n",
    "            if palabra_clave.lower() in contenido_pagina:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        # Cerrar el navegador\n",
    "        driver.quit()\n",
    "\n",
    "def obtener_titulos_desde_csv(csv_path, limite=5):\n",
    "    # Configurar Selenium\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    try:\n",
    "        # Crear una lista para almacenar los datos\n",
    "        datos = []\n",
    "\n",
    "        with open(csv_path, mode='r', encoding='utf-8') as archivo_csv:\n",
    "            lector_csv = csv.reader(archivo_csv)\n",
    "            next(lector_csv)  # Saltar la primera fila (encabezado)\n",
    "\n",
    "            for indice, fila in enumerate(lector_csv, start=1):\n",
    "                enlace = fila[1]  # La columna 1 contiene los enlaces\n",
    "                titulo = fila[0]  # La columna 0 contiene los títulos\n",
    "\n",
    "                # Verificar si el contenido de la página contiene palabras clave\n",
    "                if buscar_palabras_clave_en_pagina(enlace, palabras_clave):\n",
    "                    # Agregar los datos a la lista\n",
    "                    datos.append([titulo, enlace])\n",
    "\n",
    "                # Detener el procesamiento después de las primeras 5 filas\n",
    "                if indice >= limite:\n",
    "                    break\n",
    "\n",
    "        # Imprimir los resultados\n",
    "        for resultado in datos:\n",
    "            print(\"Título:\", resultado[0])\n",
    "            print(\"Enlace:\", resultado[1])\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    finally:\n",
    "        # Cerrar el navegador\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    palabras_clave = [\"Messi\"]  # Personaliza las palabras clave según tus necesidades\n",
    "    csv_path = 'resultados.csv'  # Ruta del archivo CSV\n",
    "    limite_filas = 5  # Número de filas a procesar\n",
    "\n",
    "    obtener_titulos_desde_csv(csv_path, limite=limite_filas)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
